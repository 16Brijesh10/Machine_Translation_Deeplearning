{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import tensorflow.strings as tf_strings\n",
    "import tensorflow.data as tf_data\n",
    "import re\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO see the GPU in system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4337202624739111990\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download And Prepare the File(Data set)\n",
    "source :\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: This block of code downloads a zip file containing the parallel corpus from a specified URL and extracts it.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "fname: The local filename to save the downloaded file as.\n",
    "origin: The URL from which to download the file.\n",
    "extract: A boolean value indicating whether to extract the contents of the zip file.\n",
    "Example:\n",
    "The zip file spa-eng.zip contains parallel text data in English and Spanish. After extraction, the contents will be available in the directory where the zip file was downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the text file: C:\\Users\\brije\\.keras\\datasets\\spa-eng\\spa.txt\n"
     ]
    }
   ],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname= \"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True\n",
    "    \n",
    ")\n",
    "# Creating the Path to the Extracted Text File\n",
    "\n",
    "text_file=pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n",
    "print(f\"Path to the text file: {text_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and Splitting the File into Lines\n",
    "\n",
    "Purpose: Reads the entire content of the text file and splits it into individual lines.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "open(text_file, \"r\"): Opens the text file in read mode.\n",
    "f.read(): Reads the entire file content as a single string.\n",
    ".split(\"\\n\"): Splits the string into a list of lines based on the newline character.\n",
    "[:-1]: Removes the last element of the list if it is an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file, \"r\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Pairs of English and Spanish Sentences\n",
    "Purpose: Processes each line to create pairs of English and Spanish sentences, with special tokens added to the Spanish sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    spa = \"[start] \" + spa + \" [end]\"\n",
    "    text_pairs.append((eng, spa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "text_pairs = []: Initializes an empty list to store the sentence pairs.\n",
    "\n",
    "for line in lines: Iterates over each line in the lines list.\n",
    "\n",
    "eng, spa = line.split(\"\\t\"): Splits each line into English and Spanish sentences based on the tab character.\n",
    "\n",
    "spa = \"[start] \" + spa + \" [end]\": Adds the [start] and [end] tokens to the Spanish sentence.\n",
    "\n",
    "text_pairs.append((eng, spa)): Adds the pair of sentences to the text_pairs list.\n",
    "\n",
    "example :\n",
    "\n",
    "[\"Go.\\tVe.\", \"Hi.\\tHola.\", \"Run.\\tCorre.\"]\n",
    "\n",
    "result:\n",
    "\n",
    "[(\"Go.\", \"[start] Ve. [end]\"), (\"Hi.\", \"[start] Hola. [end]\"), (\"Run.\", \"[start] Corre. [end]\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the words randomly\n",
    "Shuffling the text pairs using random.shuffle helps ensure that the data is randomly distributed, which can improve the training process for machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nullclass",
   "language": "python",
   "name": "nullclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
