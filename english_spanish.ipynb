{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation Project (English to Spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import tensorflow.strings as tf_strings\n",
    "import tensorflow.data as tf_data\n",
    "import re\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO see the GPU in system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4337202624739111990\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download And Prepare the File(Data set)\n",
    "source :\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: This block of code downloads a zip file containing the parallel corpus from a specified URL and extracts it.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "fname: The local filename to save the downloaded file as.\n",
    "origin: The URL from which to download the file.\n",
    "extract: A boolean value indicating whether to extract the contents of the zip file.\n",
    "Example:\n",
    "The zip file spa-eng.zip contains parallel text data in English and Spanish. After extraction, the contents will be available in the directory where the zip file was downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the text file: C:\\Users\\brije\\.keras\\datasets\\spa-eng\\spa.txt\n"
     ]
    }
   ],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname= \"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True\n",
    "    \n",
    ")\n",
    "# Creating the Path to the Extracted Text File\n",
    "\n",
    "text_file=pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n",
    "print(f\"Path to the text file: {text_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and Splitting the File into Lines\n",
    "\n",
    "Purpose: Reads the entire content of the text file and splits it into individual lines.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "open(text_file, \"r\"): Opens the text file in read mode.\n",
    "f.read(): Reads the entire file content as a single string.\n",
    ".split(\"\\n\"): Splits the string into a list of lines based on the newline character.\n",
    "[:-1]: Removes the last element of the list if it is an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file, \"r\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Pairs of English and Spanish Sentences\n",
    "Purpose: Processes each line to create pairs of English and Spanish sentences, with special tokens added to the Spanish sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    spa = \"[start] \" + spa + \" [end]\"\n",
    "    text_pairs.append((eng, spa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "text_pairs = []: Initializes an empty list to store the sentence pairs.\n",
    "\n",
    "for line in lines: Iterates over each line in the lines list.\n",
    "\n",
    "eng, spa = line.split(\"\\t\"): Splits each line into English and Spanish sentences based on the tab character.\n",
    "\n",
    "spa = \"[start] \" + spa + \" [end]\": Adds the [start] and [end] tokens to the Spanish sentence.\n",
    "\n",
    "text_pairs.append((eng, spa)): Adds the pair of sentences to the text_pairs list.\n",
    "\n",
    "example :\n",
    "\n",
    "[\"Go.\\tVe.\", \"Hi.\\tHola.\", \"Run.\\tCorre.\"]\n",
    "\n",
    "result:\n",
    "\n",
    "[(\"Go.\", \"[start] Ve. [end]\"), (\"Hi.\", \"[start] Hola. [end]\"), (\"Run.\", \"[start] Corre. [end]\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the words randomly\n",
    "Shuffling the text pairs using random.shuffle helps ensure that the data is randomly distributed, which can improve the training process for machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and shuffled text pairs.\n"
     ]
    }
   ],
   "source": [
    "#shuffle the words to improve the training process\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "print(\"Successfully created and shuffled text pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I was burned up by what he said.', '[start] Me tiene ardido con lo que me dijo. [end]')\n",
      "(\"He's stronger than you.\", '[start] Él es más fuerte que tú. [end]')\n",
      "('That guy has a screw loose!', '[start] ¡Ese tipo tiene suelto un tornillo! [end]')\n",
      "('I need your cooperation.', '[start] Necesito tu cooperación. [end]')\n",
      "('Have you ever spent any time in Boston?', '[start] ¿Alguna vez has pasado tiempo en Boston? [end]')\n"
     ]
    }
   ],
   "source": [
    "#Print Example Pairs\n",
    "\n",
    "for i in range(5):\n",
    "    print(text_pairs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### structure of data set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of validation samples, which is 15% of the total text pairs\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "\n",
    "# Calculate the number of training samples\n",
    "# The training samples are the remaining pairs after accounting for validation and test samples\n",
    "# Since we want the validation and test sets to be the same size, we multiply num_val_samples by 2 and subtract it from the total\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "\n",
    "# Split the text pairs into training, validation, and test sets\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
    "\n",
    "# Print out the number of pairs in each set\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### len(text_pairs):   \n",
    "Gets the total number of text pairs.\n",
    "\n",
    "0.15 * len(text_pairs): Calculates 15% of the total text pairs.\n",
    "\n",
    "int(...): Converts the result to an integer (since the number of samples must be a whole number).\n",
    "\n",
    "This gives the number of samples to be used for validation.\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2 * num_val_samples: \n",
    "Since we want the validation and test sets to be of equal size, we multiply the number of validation samples by 2.\n",
    "\n",
    "len(text_pairs) - 2 * num_val_samples: Subtracts the combined number of validation and test samples from the total number of text pairs.\n",
    "\n",
    "This gives the number of samples to be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_pairs = text_pairs[:num_train_samples]: Takes the first num_train_samples pairs for training.\n",
    "\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]: Takes the next num_val_samples pairs for validation.\n",
    "\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]: Takes the remaining pairs for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to preprocess Spanish text data by converting it to lowercase and removing specified characters (stored in strip_chars)\n",
    "\n",
    "tf_strings.lower(input_string): Converts the input string to lowercase.\n",
    "\n",
    "tf_strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\"): Removes characters listed in strip_chars from the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_string):\n",
    "    lowercase = tf_strings.lower(input_string)\n",
    "    return tf_strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization Layers\n",
    "Text vectorization converts text into sequences of integers, which can be used as input to machine learning models. Two TextVectorization layers are created: one for English and one for Spanish.\n",
    "\n",
    "#### English Vectorization\n",
    "\n",
    "max_tokens: The maximum size of the vocabulary.\n",
    "\n",
    "output_mode: How the output should be represented, \"int\" means output will be integer indices.\n",
    "\n",
    "output_sequence_length: The fixed length of the output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English Vectorization\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spanish Vectorization\n",
    "\n",
    "Similar to English vectorization, but with standardize set to custom_standardization, which preprocesses the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_vectorization = TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length + 1,\n",
    "    standardize = custom_standardization,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data\n",
    "Extract the text pairs for training.\n",
    "\n",
    "train_eng_texts: List of English sentences from train_pairs.\n",
    "\n",
    "train_spa_texts: List of Spanish sentences from train_pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_spa_texts = [pair[1] for pair in train_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting Vectorization Layers\n",
    "Adapt the vectorization layers to the training data.\n",
    "\n",
    "This process builds the vocabulary and prepares the layers for converting text to integer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vectorization.adapt(train_eng_texts)\n",
    "spa_vectorization.adapt(train_spa_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Vectorization Layers\n",
    "Save the configurations and vocabularies of the vectorization layers for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_config(): Gets the configuration of the vectorization layer.\n",
    "\n",
    "get_vocabulary(): Gets the vocabulary of the vectorization layer.\n",
    "\n",
    "Save the configuration and vocabulary to JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English\n",
    "eng_vectorization_config = eng_vectorization.get_config()\n",
    "eng_vectorization_config.pop('standardize', None)\n",
    "eng_vocab = eng_vectorization.get_vocabulary()\n",
    "\n",
    "with open('eng_vectorization_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(eng_vectorization_config, f)\n",
    "\n",
    "with open('eng_vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(eng_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spanish\n",
    "spa_vectorization_config = spa_vectorization.get_config()\n",
    "spa_vectorization_config.pop('standardize', None)\n",
    "spa_vocab = spa_vectorization.get_vocabulary()\n",
    "\n",
    "with open('spa_vectorization_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(spa_vectorization_config, f)\n",
    "\n",
    "with open('spa_vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(spa_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english:\n",
      "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']\n",
      "\n",
      "spanish:\n",
      "['', '[UNK]', '[start]', '[end]', 'de', 'que', 'a', 'no', 'tom', 'la']\n"
     ]
    }
   ],
   "source": [
    "print(\"english:\")\n",
    "print(eng_vocab[:10])\n",
    "print(\"\\nspanish:\")\n",
    "print(spa_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nullclass",
   "language": "python",
   "name": "nullclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
