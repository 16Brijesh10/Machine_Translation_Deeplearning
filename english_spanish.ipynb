{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import tensorflow.strings as tf_strings\n",
    "import tensorflow.data as tf_data\n",
    "import re\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO see the GPU in system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4337202624739111990\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download And Prepare the File(Data set)\n",
    "source :\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: This block of code downloads a zip file containing the parallel corpus from a specified URL and extracts it.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "fname: The local filename to save the downloaded file as.\n",
    "origin: The URL from which to download the file.\n",
    "extract: A boolean value indicating whether to extract the contents of the zip file.\n",
    "Example:\n",
    "The zip file spa-eng.zip contains parallel text data in English and Spanish. After extraction, the contents will be available in the directory where the zip file was downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the text file: C:\\Users\\brije\\.keras\\datasets\\spa-eng\\spa.txt\n"
     ]
    }
   ],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname= \"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True\n",
    "    \n",
    ")\n",
    "# Creating the Path to the Extracted Text File\n",
    "\n",
    "text_file=pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n",
    "print(f\"Path to the text file: {text_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and Splitting the File into Lines\n",
    "\n",
    "Purpose: Reads the entire content of the text file and splits it into individual lines.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "open(text_file, \"r\"): Opens the text file in read mode.\n",
    "f.read(): Reads the entire file content as a single string.\n",
    ".split(\"\\n\"): Splits the string into a list of lines based on the newline character.\n",
    "[:-1]: Removes the last element of the list if it is an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file, \"r\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Pairs of English and Spanish Sentences\n",
    "Purpose: Processes each line to create pairs of English and Spanish sentences, with special tokens added to the Spanish sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    spa = \"[start] \" + spa + \" [end]\"\n",
    "    text_pairs.append((eng, spa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "text_pairs = []: Initializes an empty list to store the sentence pairs.\n",
    "\n",
    "for line in lines: Iterates over each line in the lines list.\n",
    "\n",
    "eng, spa = line.split(\"\\t\"): Splits each line into English and Spanish sentences based on the tab character.\n",
    "\n",
    "spa = \"[start] \" + spa + \" [end]\": Adds the [start] and [end] tokens to the Spanish sentence.\n",
    "\n",
    "text_pairs.append((eng, spa)): Adds the pair of sentences to the text_pairs list.\n",
    "\n",
    "example :\n",
    "\n",
    "[\"Go.\\tVe.\", \"Hi.\\tHola.\", \"Run.\\tCorre.\"]\n",
    "\n",
    "result:\n",
    "\n",
    "[(\"Go.\", \"[start] Ve. [end]\"), (\"Hi.\", \"[start] Hola. [end]\"), (\"Run.\", \"[start] Corre. [end]\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the words randomly\n",
    "Shuffling the text pairs using random.shuffle helps ensure that the data is randomly distributed, which can improve the training process for machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and shuffled text pairs.\n"
     ]
    }
   ],
   "source": [
    "#shuffle the words to improve the training process\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "print(\"Successfully created and shuffled text pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I was burned up by what he said.', '[start] Me tiene ardido con lo que me dijo. [end]')\n",
      "(\"He's stronger than you.\", '[start] Él es más fuerte que tú. [end]')\n",
      "('That guy has a screw loose!', '[start] ¡Ese tipo tiene suelto un tornillo! [end]')\n",
      "('I need your cooperation.', '[start] Necesito tu cooperación. [end]')\n",
      "('Have you ever spent any time in Boston?', '[start] ¿Alguna vez has pasado tiempo en Boston? [end]')\n"
     ]
    }
   ],
   "source": [
    "#Print Example Pairs\n",
    "\n",
    "for i in range(5):\n",
    "    print(text_pairs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### structure of data set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of validation samples, which is 15% of the total text pairs\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "\n",
    "# Calculate the number of training samples\n",
    "# The training samples are the remaining pairs after accounting for validation and test samples\n",
    "# Since we want the validation and test sets to be the same size, we multiply num_val_samples by 2 and subtract it from the total\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "\n",
    "# Split the text pairs into training, validation, and test sets\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
    "\n",
    "# Print out the number of pairs in each set\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### len(text_pairs):   \n",
    "Gets the total number of text pairs.\n",
    "\n",
    "0.15 * len(text_pairs): Calculates 15% of the total text pairs.\n",
    "\n",
    "int(...): Converts the result to an integer (since the number of samples must be a whole number).\n",
    "\n",
    "This gives the number of samples to be used for validation.\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2 * num_val_samples: \n",
    "Since we want the validation and test sets to be of equal size, we multiply the number of validation samples by 2.\n",
    "\n",
    "len(text_pairs) - 2 * num_val_samples: Subtracts the combined number of validation and test samples from the total number of text pairs.\n",
    "\n",
    "This gives the number of samples to be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_pairs = text_pairs[:num_train_samples]: Takes the first num_train_samples pairs for training.\n",
    "\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]: Takes the next num_val_samples pairs for validation.\n",
    "\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]: Takes the remaining pairs for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nullclass",
   "language": "python",
   "name": "nullclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
